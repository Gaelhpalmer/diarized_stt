{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Online Rolling-Buffer/Chunk-Based Diarization & STT Pipeline\n",
    "Adapted from Juanma Coria's <a href='https://betterprogramming.pub/color-your-captions-streamlining-live-transcriptions-with-diart-and-openais-whisper-6203350234ef'>Blog Post</a> \"Color Your Captions: Streamlining Live Transcriptions with \"Diart\" and OpenAI's Whisper\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The pipeline incorporates both Diart: an \"<a href='github.com/juanmc2005/diart/blob/main/paper.pdf'>Overlap-Aware Low-Latency Online Speaker Diarization Based on End-to-End Local Segmentation</a>\", and Whisper from OpenAI. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing the dtw module. When using in academic works please cite:\n",
      "  T. Giorgino. Computing and Visualizing Dynamic Time Warping Alignments in R: The dtw Package.\n",
      "  J. Stat. Soft., doi:10.18637/jss.v031.i07.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Imports:\n",
    "\n",
    "# general\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import traceback\n",
    "import numpy as np\n",
    "\n",
    "# diarization module\n",
    "from diart import SpeakerDiarizationConfig, SpeakerDiarization\n",
    "from diart.sources import AppleDeviceAudioSource, FileAudioSource, MicrophoneAudioSource\n",
    "\n",
    "# asr module\n",
    "import whisper_timestamped as whisper\n",
    "from pyannote.core import Segment\n",
    "from contextlib import contextmanager\n",
    "\n",
    "# chain operators\n",
    "import diart.operators as dops\n",
    "import rich\n",
    "import rx.operators as ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diarization Module\n",
    "\n",
    "- Streaming using buffered chunks presents a unique challenge, but also brings with it unique benefits over offline diarization/transcription pipelines. Offline models can suffer from long processing times, particularly on longer conversations. Diart can handle continuous audio streaming with low and constant memory costs, and also improves performance as the stream goes on (due to continual learning through improved speaker centroids). However, DER can be quite high at the onset of streaming!!! We have to keep this in mind as we develop our downstream pipeline\n",
    "\n",
    "<strong>Diart Methods:</strong>\n",
    "* Segmentation\n",
    "    - end-to-end speaker segmentation model used to produce local speaker activity probabilites for each frame. \n",
    "    - Tau_active threshold controls min prob to be tagged in chunk\n",
    "\n",
    "* Incremental Clustering\n",
    "    - Segmentation-Driven Speaker Embedding: using modified x-vector based TDNN-based architecture with a statistical pooling layer that weighs frames based on speaker activity probs.\n",
    "    - Constrained Incremental Clustering: ensures no two local speakers are assigned the same global speaker & handles overlapping speech.\n",
    "    - Detection of New Speakers and Centroid Updates: Based on Delta_new threshold for new speakers, and centroids are only updated if the active duratiuon exceeds Rho_update.\n",
    "\n",
    "* Latency Adjustment: initial buffer must be 5s, later buffers can be set to a min value of 500 ms for heightened responsiveness (λ is Latency!).\n",
    "    - When longer latency (λ) is permitted, several positions of the rolling buffer can be combined in an ensemble-like manner (ideally improving accuracy!!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppressing whisper-timestamped warnings for clean output\n",
    "logging.getLogger(\"whisper_timestamped\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.2.5. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../../.cache/torch/pyannote/models--pyannote--segmentation/snapshots/660b9e20307a2b0cdb400d0f80aadc04a701fc54/pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.2.0. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.3.1. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.2.7 to v2.2.5. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../../.cache/torch/pyannote/models--pyannote--embedding/snapshots/4db4899737a38b2d618bbd74350915aa10293cb2/pytorch_model.bin`\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.2.7 to v2.2.5. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../../.cache/torch/pyannote/models--pyannote--embedding/snapshots/4db4899737a38b2d618bbd74350915aa10293cb2/pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.2.0. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.8.1+cu102, yours is 2.3.1. Bad things might happen unless you revert torch to 1.x.\n",
      "Model was trained with pyannote.audio 0.0.1, yours is 3.2.0. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.8.1+cu102, yours is 2.3.1. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    }
   ],
   "source": [
    " # set configurations for the speakerdiarization module\n",
    "config = SpeakerDiarizationConfig( #Parameters from Juanma - FINE-TUNE THEM!\n",
    "    duration=5, # Chunk duration in s - default: 5\n",
    "    step=0.5, # Sliding window step in s - default -0.5\n",
    "    latency=\"min\", # System latency in s \n",
    "    tau_active=0.5, \n",
    "    rho_update=0.1,\n",
    "    delta_new=0.57\n",
    ")\n",
    "\n",
    "# construct diarizer pipeline & set source\n",
    "diarizer = SpeakerDiarization(config)\n",
    "source = FileAudioSource(\"/Users/gael/Desktop/WorkFiles/ToyProjects/diarized_stt/data/3.wav\", sample_rate=config.sample_rate) #ADJUST! - Does not work.\n",
    "# For active listening demo set the source to: \n",
    "# source = MicrophoneAudioSource(sample_rate=config.sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: Tuning Parameters - From Coria Et. Al.'s 2021  <a href='github.com/juanmc2005/diart/blob/main/paper.pdf'>Paper</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Tau_active: tunable speaker activity probability threshold (0-1). (Speakers whose activity exceeds Tau_active during a chunk constitute the set of local speakers, their activity probabilities are then passed downstream to the incremental clustering step--thus handling the overlapping-speech problem fromt the start as opposed to in post-processing).\n",
    "\n",
    "- Rho_update: tunable parameter controlling the rate at which speaker embeddings are updated (seconds). (Similar to/represents the learning rate)\n",
    "\n",
    "- Delta_new: tunable parameter controlling the threshold for considering new speakers in a chunk (0-2). Lower values will make the system more sensitive to different voices (Mathematically represents the minimum distance required between embeddings for the clusters to be considered seperable)\n",
    "\n",
    "<a href='github.com/juanmc2005/diart/blob/main/src/diart/console/tune.py'>diart.tune</a> - can be used to automatically tune these parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whisper/ASR Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class definition\n",
    "@contextmanager\n",
    "def suppress_stdout():\n",
    "    # Aux function to suppress (extremely long) whisper logs (https://thesmithfam.org/blog/2012/10/25/temporarily-suppress-console-output-in-python/)\n",
    "    with open(os.devnull, \"w\") as devnull: # open the devnull file (discards all data written to it!)\n",
    "        old_stdout = sys.stdout # save current std output (to restore later)\n",
    "        sys.stdout = devnull # redirect std output to devnull (discarding any logs)\n",
    "        try: \n",
    "            yield # temporarily exit context manager, allowing wrapped code to execute with the suppressed std output\n",
    "        finally: # ensure original stdout is restored even if an exception occures (don't wanna play with that!!)\n",
    "            sys.stdout = old_stdout \n",
    "\n",
    "class WhisperTranscriber:\n",
    "    def __init__(self, model=\"small\", device=None):\n",
    "        self.model = whisper.load_model(model, device=device)\n",
    "        self._buffer = \"\"\n",
    "\n",
    "    def transcribe(self, waveform):\n",
    "        \"\"\"Transcribe audio using Whisper\"\"\"\n",
    "        # Pad/trim audio to fit 30 seconds as required by Whisper\n",
    "        audio = waveform.data.astype(\"float32\").reshape(-1)\n",
    "        audio = whisper.pad_or_trim(audio)\n",
    "\n",
    "        # Transcribe the given audio while suppressing logs\n",
    "        with suppress_stdout():\n",
    "            transcription = whisper.transcribe(\n",
    "                self.model,\n",
    "                audio,\n",
    "                # We use past transcriptions to condition the model\n",
    "                initial_prompt=self._buffer,\n",
    "                verbose=True  # to avoid progress bar\n",
    "            )\n",
    "\n",
    "        return transcription\n",
    "\n",
    "    def identify_speakers(self, transcription, diarization, time_shift):\n",
    "        \"\"\"Iterate over transcription segments to assign speakers\"\"\"\n",
    "        speaker_captions = []\n",
    "        for segment in transcription[\"segments\"]:\n",
    "\n",
    "            # Crop diarization to the segment timestamps\n",
    "            start = time_shift + segment[\"words\"][0][\"start\"]\n",
    "            end = time_shift + segment[\"words\"][-1][\"end\"]\n",
    "            dia = diarization.crop(Segment(start, end))\n",
    "\n",
    "            # Assign a speaker to the segment based on diarization\n",
    "            speakers = dia.labels()\n",
    "            num_speakers = len(speakers)\n",
    "            if num_speakers == 0:\n",
    "                # No speakers were detected\n",
    "                caption = (-1, segment[\"text\"])\n",
    "            elif num_speakers == 1:\n",
    "                # Only one speaker is active in this segment\n",
    "                spk_id = int(speakers[0].split(\"speaker\")[1])\n",
    "                caption = (spk_id, segment[\"text\"])\n",
    "            else:\n",
    "                # Multiple speakers, select the one that speaks the most\n",
    "                max_speaker = int(np.argmax([\n",
    "                    dia.label_duration(spk) for spk in speakers\n",
    "                ]))\n",
    "                caption = (max_speaker, segment[\"text\"])\n",
    "            speaker_captions.append(caption)\n",
    "\n",
    "        return speaker_captions\n",
    "\n",
    "    def __call__(self, diarization, waveform):\n",
    "        # Step 1: Transcribe\n",
    "        transcription = self.transcribe(waveform)\n",
    "        # Update transcription buffer\n",
    "        self._buffer += transcription[\"text\"]\n",
    "        # The audio may not be the beginning of the conversation\n",
    "        time_shift = waveform.sliding_window.start\n",
    "        # Step 2: Assign speakers\n",
    "        speaker_transcriptions = self.identify_speakers(transcription, diarization, time_shift)\n",
    "        return speaker_transcriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr = WhisperTranscriber"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Both Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyannote.core import Annotation, SlidingWindowFeature, SlidingWindow\n",
    "\n",
    "def concat(chunks, collar=0.05):\n",
    "    \"\"\"\n",
    "    Concatenate predictions and audio\n",
    "    given a list of `(diarization, waveform)` pairs\n",
    "    and merge contiguous single-speaker regions\n",
    "    with pauses shorter than `collar` seconds.\n",
    "    \"\"\"\n",
    "    first_annotation = chunks[0][0]\n",
    "    first_waveform = chunks[0][1]\n",
    "    annotation = Annotation(uri=first_annotation.uri)\n",
    "    data = []\n",
    "    for ann, wav in chunks:\n",
    "        annotation.update(ann)\n",
    "        data.append(wav.data)\n",
    "    annotation = annotation.support(collar)\n",
    "    window = SlidingWindow(\n",
    "        first_waveform.sliding_window.duration,\n",
    "        first_waveform.sliding_window.step,\n",
    "        first_waveform.sliding_window.start,\n",
    "    )\n",
    "    data = np.concatenate(data, axis=0)\n",
    "    return annotation, SlidingWindowFeature(data, window)\n",
    "\n",
    "def colorize_transcription(transcription):\n",
    "    \"\"\"\n",
    "    Unify a speaker-aware transcription represented as\n",
    "    a list of `(speaker: int, text: str)` pairs\n",
    "    into a single text colored by speakers.\n",
    "    \"\"\"\n",
    "    colors = 2 * [\n",
    "        \"bright_red\", \"bright_blue\", \"bright_green\", \"orange3\", \"deep_pink1\",\n",
    "        \"yellow2\", \"magenta\", \"cyan\", \"bright_magenta\", \"dodger_blue2\"\n",
    "    ]\n",
    "    result = []\n",
    "    for speaker, text in transcription:\n",
    "        if speaker == -1:\n",
    "            # No speakerfound for this text, use default terminal color\n",
    "            result.append(text)\n",
    "        else:\n",
    "            result.append(f\"[{colors[speaker]}]{text}\")\n",
    "    return \"\\n\".join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<rx.disposable.disposable.Disposable at 0x2a047dc70>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the stream into 2s chunks for transcription\n",
    "transcription_duration = 2\n",
    "# Apply models in batches for better efficiency\n",
    "batch_size = int(transcription_duration // config.step)\n",
    "\n",
    "# Chain of operations to apply on the stream of microphone audio\n",
    "source.stream.pipe(\n",
    "    # Format audio stream to sliding windows of 5s with a step of 500ms\n",
    "    dops.rearrange_audio_stream(\n",
    "        config.duration, config.step, config.sample_rate\n",
    "    ),\n",
    "    # Wait until a batch is full\n",
    "    # The output is a list of audio chunks\n",
    "    ops.buffer_with_count(count=batch_size),\n",
    "    # Obtain diarization prediction\n",
    "    # The output is a list of pairs `(diarization, audio chunk)`\n",
    "    ops.map(diarizer),\n",
    "    # Concatenate 500ms predictions/chunks to form a single 2s chunk\n",
    "    ops.map(concat),\n",
    "    # Ignore this chunk if it does not contain speech\n",
    "    ops.filter(lambda ann_wav: ann_wav[0].get_timeline().duration() > 0),\n",
    "    # Obtain speaker-aware transcriptions\n",
    "    # The output is a list of pairs `(speaker: int, caption: str)`\n",
    "    ops.starmap(asr),\n",
    "    # Color transcriptions according to the speaker\n",
    "    # The output is plain text with color references for rich\n",
    "    ops.map(colorize_transcription),\n",
    ").subscribe(\n",
    "    on_next=rich.print,  # print colored text\n",
    "    on_error=lambda _: traceback.print_exc()  # print stacktrace if error\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "listening...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/gael/miniconda3/envs/ml_env/lib/python3.8/site-packages/rx/core/operators/map.py\", line 37, in on_next\n",
      "    result = _mapper(value)\n",
      "  File \"/Users/gael/miniconda3/envs/ml_env/lib/python3.8/site-packages/rx/operators/__init__.py\", line 2662, in <lambda>\n",
      "    return pipe(map(lambda values: cast(Mapper, mapper)(*values)))\n",
      "  File \"/var/folders/15/wd23przs6r1gtq0g6ks4kvkh0000gn/T/ipykernel_2807/3041333293.py\", line 15, in __init__\n",
      "    self.model = whisper.load_model(model, device=device)\n",
      "  File \"/Users/gael/miniconda3/envs/ml_env/lib/python3.8/site-packages/whisper_timestamped/transcribe.py\", line 2441, in load_model\n",
      "    extension = os.path.splitext(name)[-1] if os.path.isfile(name) else None\n",
      "  File \"/Users/gael/miniconda3/envs/ml_env/lib/python3.8/genericpath.py\", line 30, in isfile\n",
      "    st = os.stat(path)\n",
      "TypeError: stat: path should be string, bytes, os.PathLike or integer, not Annotation\n"
     ]
    }
   ],
   "source": [
    "print('listening...')\n",
    "source.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
